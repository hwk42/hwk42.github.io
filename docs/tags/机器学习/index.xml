<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>机器学习 on Walker的博客</title>
    <link>https://hwk42.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 机器学习 on Walker的博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Walker - All rights reserved</copyright>
    <lastBuildDate>Mon, 29 Nov 2021 19:30:10 +0800</lastBuildDate><atom:link href="https://hwk42.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>让机器学习更快些-在K8s中支持GPU调度</title>
      <link>https://hwk42.github.io/blog/cloudnative/%E8%AE%A9%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%9B%B4%E5%BF%AB%E4%BA%9B--%E5%9C%A8k8s%E4%B8%AD%E6%94%AF%E6%8C%81gpu%E8%B0%83%E5%BA%A6/</link>
      <pubDate>Mon, 29 Nov 2021 19:30:10 +0800</pubDate>
      
      <guid>https://hwk42.github.io/blog/cloudnative/%E8%AE%A9%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%9B%B4%E5%BF%AB%E4%BA%9B--%E5%9C%A8k8s%E4%B8%AD%E6%94%AF%E6%8C%81gpu%E8%B0%83%E5%BA%A6/</guid>
      <description>机器学习(尤其是深度学习)的训练和推理是计算密集型应用，使用CPU往往达不到性能要求，通常需要特殊的硬件如GPU、FPGA等来加速。最常见、通用的加速硬件是NVIDIA GPU， 这里记录下在K8s环境下如何支持GPU调度。支持GPU有两种方法：手动安装和使用gpu-operator安装</description>
    </item>
    
    <item>
      <title>使用Openscoring部署Spark ML Pipeline模型</title>
      <link>https://hwk42.github.io/blog/machinelearning/%E4%BD%BF%E7%94%A8openscoring%E9%83%A8%E7%BD%B2spark-ml-pipeline%E6%A8%A1%E5%9E%8B/</link>
      <pubDate>Tue, 09 Nov 2021 16:31:21 +0800</pubDate>
      
      <guid>https://hwk42.github.io/blog/machinelearning/%E4%BD%BF%E7%94%A8openscoring%E9%83%A8%E7%BD%B2spark-ml-pipeline%E6%A8%A1%E5%9E%8B/</guid>
      <description>Spark MLlib在机器学习领域应用虽不如TensorFlow、PyTorch框架受欢迎，不过其也能完成完整的机器学习流水线，而且可以与Spark其他组件无缝集成，与大数据生态互通也更有优势。这篇文章记录下使用Spark ML Pipeline 训练并部署模型过程。测试Spark版本为3.</description>
    </item>
    
    <item>
      <title>混合精度训练</title>
      <link>https://hwk42.github.io/blog/machinelearning/%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6%E8%AE%AD%E7%BB%83/</link>
      <pubDate>Wed, 19 Feb 2020 23:42:15 +0800</pubDate>
      
      <guid>https://hwk42.github.io/blog/machinelearning/%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6%E8%AE%AD%E7%BB%83/</guid>
      <description>混合精度训练可以在不改变网络架构的前提下，通过以半精度格式执行操作来显着提高计算速度，同时以单精度存储最少的信息以在网络的关键部分中保留尽可能多的信息。当前深度学习训练/推理使用的主流数值精度是32bit(即FP32)，随着深度学习商用硬件的发展，16bit(FP16/DFP16)混合精度在训练中的应用逐渐成熟，如Intel的NNP，NVIDIA的 Tensor Core。</description>
    </item>
    
  </channel>
</rss>
